{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0-Iqvsjrorf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cab7365c-a690-4a91-a716-91af256d3960"
      },
      "source": [
        "%mkdir -p /content/csc421/a3/\n",
        "%cd /content/csc421/a3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2bgC8RDfrxh",
        "colab_type": "text"
      },
      "source": [
        "# Helper Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWXwMG4BfuaI",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKtrNITJQWda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xWTqk1lQYpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjH199-KQnqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLzNKeVkQrl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK19GqC9QuGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf9Bl5IwQw7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qVr6b40f2Yd",
        "colab_type": "text"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-3XTvIvQ0pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsGLlaZ0f60V",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rMWQs6zRFsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if opts.cuda and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = torch.LongTensor(indexes).unsqueeze(0).to(device)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = torch.LongTensor([[start_token]]).to(device)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if opts.cuda and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = torch.LongTensor(indexes).unsqueeze(0).to(device)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = torch.LongTensor([[start_token]]).to(device)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if opts.cuda and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = torch.stack(input_tensors[start:end]).to(device)\n",
        "            targets = torch.stack(target_tensors[start:end]).to(device)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = start_vector.to(device)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "\n",
        "    device = torch.device(\"cuda\" if opts.cuda and torch.cuda.is_available() else \"cpu\")\n",
        "    torch.manual_seed(opts.seed)\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                         hidden_size=opts.hidden_size, \n",
        "                         opts=opts)\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    encoder.to(device)\n",
        "    decoder.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP_UKfYcgC_I",
        "colab_type": "text"
      },
      "source": [
        "# Your code for NMT models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ndr7trlgHah",
        "colab_type": "text"
      },
      "source": [
        "## GRU cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW_Sx1jcZOa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        ## Input linear layers\n",
        "        self.Wiz = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.Wir = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.Wih = nn.Linear(self.input_size, self.hidden_size)\n",
        "\n",
        "        ## Hidden linear layers\n",
        "        self.Whz = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.Whr = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.Whh = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        z = F.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        r = F.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        g = F.tanh(self.Wih(x) + r * self.Whh(h_prev))\n",
        "        h_new = (1 - z) * g + z * h_prev\n",
        "        return h_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwN9Vv5SgL-x",
        "colab_type": "text"
      },
      "source": [
        "## GRU encoder/decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjXw6sD3ZUwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size, inputs.device)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs, dev):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return torch.zeros(bs, self.hidden_size).to(dev)\n",
        "\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH0zPO5cgRnh",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go4hLWXjZZkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"\n",
        "        The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        expanded_queries = queries.unsqueeze(1).expand_as(keys)\n",
        "        concat_inputs = torch.cat((expanded_queries, keys), dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)                                                                                                          \n",
        "        attention_weights = torch.exp(F.log_softmax(unnormalized_attention, dim=1))\n",
        "        context = torch.bmm(attention_weights.transpose(1, 2), values)\n",
        "        return context, attention_weights\n",
        "        \n",
        "      \n",
        "\n",
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        \n",
        "        if queries.dim() < 3:\n",
        "          queries = queries.unsqueeze(1)\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = torch.bmm(q, k.transpose(1, 2)) * self.scaling_factor # bsz x k x seq_len\n",
        "        attention_weights = torch.exp(F.log_softmax(unnormalized_attention, dim=-1)) # bsz x k x seq_len\n",
        "        context = torch.bmm(attention_weights, values) # bsz x k x hsz\n",
        "        return context, attention_weights.transpose(1, 2)\n",
        "\n",
        "      \n",
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e9)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        if queries.dim() < 3:\n",
        "          queries = queries.unsqueeze(1)\n",
        "        _, mid, _ = queries.size()\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = torch.bmm(q, k.transpose(1, 2)) * self.scaling_factor # bsz x k x seq_len\n",
        "        mask = torch.tril(torch.ones(unnormalized_attention.size(), dtype=torch.uint8)).to(unnormalized_attention.device)\n",
        "        x = unnormalized_attention * mask\n",
        "        y = torch.Tensor(mask.size()).fill_(self.neg_inf).to(mask.device)\n",
        "        unnormalized_attention = torch.where(x == 0, y, x)\n",
        "        attention_weights = torch.exp(F.log_softmax(unnormalized_attention, dim=-1)) # bsz x k x seq_len\n",
        "        context = torch.bmm(attention_weights, values) # bsz x k x hsz\n",
        "        return context, attention_weights.transpose(1, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GC-dWAgVx5",
        "colab_type": "text"
      },
      "source": [
        "## Attention decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOzvGZz7Z93N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            embed_current = embed[:, i, :]\n",
        "            context, attention_weights = self.attention(h_prev, annotations, annotations)\n",
        "            embed_and_context = torch.cat((embed_current, context.squeeze(1)), dim=-1)\n",
        "            h_prev = self.rnn(embed_and_context, h_prev)\n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npZSULF0gZs-",
        "colab_type": "text"
      },
      "source": [
        "## Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKQndhFaaEyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init=None):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x decoder_seq_len x hidden_size        \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed \n",
        "        for i in range(self.num_layers):\n",
        "          new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)\n",
        "          residual_contexts = contexts + new_contexts\n",
        "          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations)\n",
        "          residual_contexts = residual_contexts + new_contexts\n",
        "          new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "          contexts = new_contexts + residual_contexts\n",
        "\n",
        "          \n",
        "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "          self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH-OnFkegd3V",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoRaxfYMgfWf",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KNYR-RiaLfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6669c77-716c-4a29-9194-2d59c46b73a0"
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG3Id6DbgiZi",
        "colab_type": "text"
      },
      "source": [
        "## RNN decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4M3hGvAaN8Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9cb22744-9caa-4900-df60-91019fe3b743"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "              'seed': 20200212\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "                                   seed: 20200212                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('utterance', 'utteranceway')\n",
            "('poor', 'oorpay')\n",
            "('charm', 'armchay')\n",
            "('constitutional', 'onstitutionalcay')\n",
            "('promised', 'omisedpray')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Epoch:   0 | Train loss: 2.299 | Val loss: 1.981 | Gen: ingsay ay-ay-ay-ay-ay-ay-ay ongway ingsay ongway\n",
            "Epoch:   1 | Train loss: 1.873 | Val loss: 1.862 | Gen: ingsay ay-ay-ay-ay-ay-ay-ay ongway ingsay ongway\n",
            "Epoch:   2 | Train loss: 1.759 | Val loss: 1.770 | Gen: eray-ay-ay-ay-ay-ay- ay-ay-ay-ay-ay-ay-ay onterway eray-ay-ay-ay-ay-ay- onsay-ay-ay-ay-ay-ay\n",
            "Epoch:   3 | Train loss: 1.665 | Val loss: 1.724 | Gen: eay-ay-ay-ay-ay-ay-a ay-ay-ay-ay-ay-ay-ay onteronsay-ay-ay-ay- ingway-ay-ay-ay-ay-a onsay-ingway-ay-ay-a\n",
            "Epoch:   4 | Train loss: 1.597 | Val loss: 1.689 | Gen: eway-ay-ay-ay-ay-ay- ay-ay-ay-ay-ay-ay-ay overonteray-orway-ay ingway-ay-ay-ay-ay-a onteray-ortay-ingray\n",
            "Epoch:   5 | Train loss: 1.538 | Val loss: 1.647 | Gen: eway-ay-ay-ay-ay-ay- ayway-ay-ay-ay-ay-ay oventeray-ortay-ingw ingway-ay-ay-ay-ay-a onteray-ortedway\n",
            "Epoch:   6 | Train loss: 1.481 | Val loss: 1.637 | Gen: eay-ay-ay-ay-ay-ay-a ayway-ay-ay-ay-ay-ay oventerteray-ortedwa isedway onterteray-ingray-in\n",
            "Epoch:   7 | Train loss: 1.431 | Val loss: 1.634 | Gen: esedway away-ay-ay-ay-ay-ay- onterouthontay-inted isingway-ayday orteray-ingray-ingha\n",
            "Epoch:   8 | Train loss: 1.385 | Val loss: 1.570 | Gen: eway ay-ay-ay-ay-ay-ay-ay ontingray-incorteray issay-ayday orteray-orationsay\n",
            "Epoch:   9 | Train loss: 1.336 | Val loss: 1.537 | Gen: eway away-ay-ay-ay-ay-ay- ontionsay-ingray-ayd isway oroushedway\n",
            "Epoch:  10 | Train loss: 1.307 | Val loss: 1.502 | Gen: eway ay-ay-ay-ay-ay-ay-ay ontionray-ingray-ay- isway overay-ay-ay-ay-ay-a\n",
            "Epoch:  11 | Train loss: 1.286 | Val loss: 1.561 | Gen: eway-ayday away-ayday ortiontray-intedway isway-ayday orrortay-ingray\n",
            "Epoch:  12 | Train loss: 1.260 | Val loss: 1.491 | Gen: estay-ayday away-ayday ontitionsay-ingray-a isway-awlay ordorday-ingray\n",
            "Epoch:  13 | Train loss: 1.214 | Val loss: 1.427 | Gen: esway ay-away-ingway oningionsay-ingray isway orourway-awlay\n",
            "Epoch:  14 | Train loss: 1.188 | Val loss: 1.469 | Gen: estway away-away-awlay oningingingway-awlay isway-ayday orouray-ayday\n",
            "Epoch:  15 | Train loss: 1.175 | Val loss: 1.403 | Gen: estway aidway oningionsingway isway-ayday orouray-ingway\n",
            "Epoch:  16 | Train loss: 1.149 | Val loss: 1.448 | Gen: esway ay-away-away-ayday oningitingway isway-ayday orconcay\n",
            "Epoch:  17 | Train loss: 1.135 | Val loss: 1.383 | Gen: estway aidway oningionsingway isway-ayday orouray-ingway\n",
            "Epoch:  18 | Train loss: 1.116 | Val loss: 1.384 | Gen: estway aiway oningitingway isway ovordway\n",
            "Epoch:  19 | Train loss: 1.104 | Val loss: 1.448 | Gen: estway aidway oningioningway-inway iway-ayday orourtedway\n",
            "Epoch:  20 | Train loss: 1.086 | Val loss: 1.350 | Gen: estway airway onciencionsingway iway-awlay orourcay\n",
            "Epoch:  21 | Train loss: 1.068 | Val loss: 1.376 | Gen: estway airway ointioncingway iway-ayday orourcay-ayday\n",
            "Epoch:  22 | Train loss: 1.055 | Val loss: 1.321 | Gen: estway airway oningionsingway iway-ayday orourcay\n",
            "Epoch:  23 | Train loss: 1.040 | Val loss: 1.361 | Gen: estway airway onincoiningingcay iway-ay orourcay-ayday\n",
            "Epoch:  24 | Train loss: 1.029 | Val loss: 1.302 | Gen: estway airway onioningsay-ingray iway-ayday orouray-oray-oray\n",
            "Epoch:  25 | Train loss: 1.006 | Val loss: 1.325 | Gen: estway airway ontioningsionsway iway-ay orourtay-ayday\n",
            "Epoch:  26 | Train loss: 0.994 | Val loss: 1.304 | Gen: estway airway oningioningsay iway-ybay ororourway\n",
            "Epoch:  27 | Train loss: 1.011 | Val loss: 1.412 | Gen: essway airway ontisioningway iway-ayday orouscay\n",
            "Epoch:  28 | Train loss: 1.001 | Val loss: 1.399 | Gen: ethay away-inway oningisingway isway ordornessway\n",
            "Epoch:  29 | Train loss: 1.001 | Val loss: 1.302 | Gen: estway aray-away onintioningway-acked iway-ayday orouspay-ayday\n",
            "Epoch:  30 | Train loss: 0.968 | Val loss: 1.254 | Gen: ethay airway ontisioningcay isway orordedway\n",
            "Epoch:  31 | Train loss: 0.944 | Val loss: 1.285 | Gen: estway airway ontionionsingway isway ororousway\n",
            "Epoch:  32 | Train loss: 0.936 | Val loss: 1.286 | Gen: estway airway ontisioningway isway ordionway\n",
            "Epoch:  33 | Train loss: 0.937 | Val loss: 1.340 | Gen: estway arainway ontionionsingway isway ororoussay\n",
            "Epoch:  34 | Train loss: 0.935 | Val loss: 1.289 | Gen: ethay airway ontisioningway isway ordionway\n",
            "Epoch:  35 | Train loss: 0.928 | Val loss: 1.310 | Gen: ethay airway ontingionsingway isway orousingway\n",
            "Epoch:  36 | Train loss: 0.913 | Val loss: 1.301 | Gen: estway airway onticingray-away-ays isway ordingcay\n",
            "Epoch:  37 | Train loss: 0.909 | Val loss: 1.274 | Gen: ethay arayway ontionionsway isway orousionway\n",
            "Epoch:  38 | Train loss: 0.898 | Val loss: 1.295 | Gen: ethay away-ybay ontisiondingway isway ordionsway\n",
            "Epoch:  39 | Train loss: 0.888 | Val loss: 1.229 | Gen: estway arighay ontionionsway isway orousionway\n",
            "Epoch:  40 | Train loss: 0.874 | Val loss: 1.326 | Gen: ethay airway ontiontionsway isway ordingday\n",
            "Epoch:  41 | Train loss: 0.880 | Val loss: 1.248 | Gen: ethay arighay ontingionsingway isway orordingway\n",
            "Epoch:  42 | Train loss: 0.873 | Val loss: 1.343 | Gen: ethay airway onticingsay isway ordingway\n",
            "Epoch:  43 | Train loss: 0.874 | Val loss: 1.196 | Gen: ethay airway ontiontionway isway ordornay\n",
            "Epoch:  44 | Train loss: 0.861 | Val loss: 1.304 | Gen: ethay airway onticingingway isway orday-orday\n",
            "Epoch:  45 | Train loss: 0.876 | Val loss: 1.301 | Gen: ethay arighay onticionsingway issway ordionsway\n",
            "Epoch:  46 | Train loss: 0.871 | Val loss: 1.264 | Gen: ethay airway onticingsay isway ordingday\n",
            "Epoch:  47 | Train loss: 0.842 | Val loss: 1.260 | Gen: ethay airway ontionionsway isway orordingway\n",
            "Epoch:  48 | Train loss: 0.832 | Val loss: 1.243 | Gen: ethay airway ontionionsway isway orderday\n",
            "Epoch:  49 | Train loss: 0.821 | Val loss: 1.206 | Gen: ethay airway ontionionspay isway orordingday\n",
            "Epoch:  50 | Train loss: 0.815 | Val loss: 1.261 | Gen: ethay airway onticingsingway isway ordedbay\n",
            "Epoch:  51 | Train loss: 0.813 | Val loss: 1.176 | Gen: ethay airway ontionionsay-indway isway orordingday\n",
            "Epoch:  52 | Train loss: 0.810 | Val loss: 1.263 | Gen: ethay airway ontingiondsway isway orondway\n",
            "Epoch:  53 | Train loss: 0.819 | Val loss: 1.239 | Gen: ethay airway ontionionsway isway orordingday\n",
            "Epoch:  54 | Train loss: 0.799 | Val loss: 1.301 | Gen: etay airway onticingsingway isway orderdsay\n",
            "Epoch:  55 | Train loss: 0.803 | Val loss: 1.239 | Gen: ethay airway ontiontionway isway orordway\n",
            "Epoch:  56 | Train loss: 0.796 | Val loss: 1.223 | Gen: ethay airway onticingsay isway ortableway\n",
            "Epoch:  57 | Train loss: 0.802 | Val loss: 1.184 | Gen: ethay airway ontiondispay isway orordway\n",
            "Epoch:  58 | Train loss: 0.782 | Val loss: 1.260 | Gen: ethay airway oncingionsingway isway ordingcay\n",
            "Epoch:  59 | Train loss: 0.785 | Val loss: 1.223 | Gen: ethay airway ontingiondspay isway orondway\n",
            "Epoch:  60 | Train loss: 0.784 | Val loss: 1.341 | Gen: eatay airway oncoritingucingway isway orondingday\n",
            "Epoch:  61 | Train loss: 0.803 | Val loss: 1.121 | Gen: ethay airway ontionionspay isway orrorindway\n",
            "Epoch:  62 | Train loss: 0.764 | Val loss: 1.173 | Gen: etay airway ontingioncay isway oronday\n",
            "Epoch:  63 | Train loss: 0.760 | Val loss: 1.227 | Gen: ethay airway onticionspray isway ordingcay\n",
            "Epoch:  64 | Train loss: 0.777 | Val loss: 1.283 | Gen: ethay airway ontiondisay-ayday isway orondsray\n",
            "Epoch:  65 | Train loss: 0.781 | Val loss: 1.189 | Gen: ethay airway onticingsay isway orderway\n",
            "Epoch:  66 | Train loss: 0.750 | Val loss: 1.180 | Gen: eathay airway onticingway isway orderday\n",
            "Epoch:  67 | Train loss: 0.737 | Val loss: 1.166 | Gen: eathay airway ontingioncay isway orderday\n",
            "Epoch:  68 | Train loss: 0.730 | Val loss: 1.157 | Gen: etay airway ontioncingway isway orbornedway\n",
            "Epoch:  69 | Train loss: 0.735 | Val loss: 1.156 | Gen: ethay airway onticousingway isway orromedway\n",
            "Epoch:  70 | Train loss: 0.788 | Val loss: 1.206 | Gen: ethay airway ontingioncay isway orondsay\n",
            "Epoch:  71 | Train loss: 0.741 | Val loss: 1.130 | Gen: ethay airway onticingway isway ordingday\n",
            "Epoch:  72 | Train loss: 0.720 | Val loss: 1.138 | Gen: eathay airway onticionsway isway ordionway\n",
            "Epoch:  73 | Train loss: 0.718 | Val loss: 1.194 | Gen: eathay airway oncinionsingway isway orderday\n",
            "Epoch:  74 | Train loss: 0.722 | Val loss: 1.175 | Gen: ethay airway onticingday isway ordingday\n",
            "Epoch:  75 | Train loss: 0.722 | Val loss: 1.194 | Gen: ethay airway oncitingway isway ordionsway\n",
            "Epoch:  76 | Train loss: 0.712 | Val loss: 1.120 | Gen: ethay airway onticousingway isway oronday-inway\n",
            "Epoch:  77 | Train loss: 0.696 | Val loss: 1.072 | Gen: ethay airway onciniotionway isway ordionsway\n",
            "Epoch:  78 | Train loss: 0.692 | Val loss: 1.084 | Gen: ethay airway onticoringway isway orordway\n",
            "Epoch:  79 | Train loss: 0.698 | Val loss: 1.063 | Gen: ethay airway ontiondisedway isway orrorcay\n",
            "Epoch:  80 | Train loss: 0.693 | Val loss: 1.090 | Gen: ethay airway onticionsingway isway ordingday\n",
            "Epoch:  81 | Train loss: 0.730 | Val loss: 1.284 | Gen: eathay airway ontingicay-ivedway isway orday-onsray\n",
            "Epoch:  82 | Train loss: 0.725 | Val loss: 1.145 | Gen: ethay airway onticingway isway ordingday\n",
            "Epoch:  83 | Train loss: 0.691 | Val loss: 1.091 | Gen: eathay airway onticingionway isway ordingday\n",
            "Epoch:  84 | Train loss: 0.677 | Val loss: 1.067 | Gen: ethay airway onticorityway isway ordingday\n",
            "Epoch:  85 | Train loss: 0.664 | Val loss: 1.033 | Gen: ethay airway onticorityway isway ordionway\n",
            "Epoch:  86 | Train loss: 0.654 | Val loss: 1.071 | Gen: ethay airway onticorityway isway ordingday\n",
            "Epoch:  87 | Train loss: 0.654 | Val loss: 1.102 | Gen: ethay airway onticionsway isway ordiondray\n",
            "Epoch:  88 | Train loss: 0.672 | Val loss: 1.069 | Gen: eathay airway onticinorway isway orderbay\n",
            "Epoch:  89 | Train loss: 0.676 | Val loss: 1.111 | Gen: ethay airway onticionsingway isway ordingday\n",
            "Epoch:  90 | Train loss: 0.680 | Val loss: 1.101 | Gen: ethay airway ontioncingway isway ordredtray\n",
            "Epoch:  91 | Train loss: 0.662 | Val loss: 1.100 | Gen: ethay airway onticingingway isway ordingday\n",
            "Epoch:  92 | Train loss: 0.647 | Val loss: 1.062 | Gen: ethay airway onticorityway isway ordingday\n",
            "Epoch:  93 | Train loss: 0.638 | Val loss: 1.069 | Gen: ethay airway onticingsay isway ordingday\n",
            "Epoch:  94 | Train loss: 0.641 | Val loss: 1.090 | Gen: ethay airway ontionditypay isway ordionsway\n",
            "Epoch:  95 | Train loss: 0.648 | Val loss: 1.091 | Gen: ethay airway onticionsray isway ordingday\n",
            "Epoch:  96 | Train loss: 0.649 | Val loss: 1.115 | Gen: ethay airway onticoringway isway ordionway\n",
            "Epoch:  97 | Train loss: 0.651 | Val loss: 1.132 | Gen: ethay airway onticionsingway isway ordionway\n",
            "Epoch:  98 | Train loss: 0.645 | Val loss: 1.035 | Gen: ethay airway ontionionscay isway ordionway\n",
            "Epoch:  99 | Train loss: 0.633 | Val loss: 1.058 | Gen: ethay airway onticionsray isway ordionway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onticionsray isway ordionway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU1dkQgmaTVi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d3bdb0c2-a8c6-48a4-9d5d-0c4a9d23959f"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onticionsray isway ordionway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu4t-ec-gmmm",
        "colab_type": "text"
      },
      "source": [
        "## RNN attention decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9-ybKY5fg1T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af89e44c-95ea-4e30-bb9e-b66b9ef656d8"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "              'seed': 20200212\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "                                   seed: 20200212                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('utterance', 'utteranceway')\n",
            "('poor', 'oorpay')\n",
            "('charm', 'armchay')\n",
            "('constitutional', 'onstitutionalcay')\n",
            "('promised', 'omisedpray')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Epoch:   0 | Train loss: 2.292 | Val loss: 1.994 | Gen: ontentay ay-ay-ay-ay-ay-ay-ay onday onsay onday\n",
            "Epoch:   1 | Train loss: 1.801 | Val loss: 1.799 | Gen: othay-othay-othay-ot aray-ay-ay-ay-ay-ay- ongsway onssay onglay\n",
            "Epoch:   2 | Train loss: 1.592 | Val loss: 1.668 | Gen: ortay-orterteray away-ay-ay-ay-ay-ay- ongsay indway onglay\n",
            "Epoch:   3 | Train loss: 1.443 | Val loss: 1.563 | Gen: etay-ay-ay-ay-ay-ay- away-ay-ay-ay-ay-ay- ongingway insway ongray-ingway-ingway\n",
            "Epoch:   4 | Train loss: 1.307 | Val loss: 1.491 | Gen: ethay-orthay-orthay- ay-ay-ay-ay-ay-ay-ay ongingay inway ongray\n",
            "Epoch:   5 | Train loss: 1.170 | Val loss: 1.376 | Gen: ortherterway away-away-away-away- ongingway-ingway-ing isway orngway-ingway-ingwa\n",
            "Epoch:   6 | Train loss: 1.046 | Val loss: 1.398 | Gen: eray ayway onfitingway isway orngway-y-y-y-y-y-y-\n",
            "Epoch:   7 | Train loss: 0.910 | Val loss: 1.230 | Gen: erththththththththth aingay onfitiningway issway orningway-y-y\n",
            "Epoch:   8 | Train loss: 0.771 | Val loss: 1.168 | Gen: eway airway-y onditingingway isway orksingway-ingway-in\n",
            "Epoch:   9 | Train loss: 0.687 | Val loss: 1.011 | Gen: eway airway ongitititiningway isway orfingway\n",
            "Epoch:  10 | Train loss: 0.591 | Val loss: 0.899 | Gen: eway airway onditingway isway orkiwlay\n",
            "Epoch:  11 | Train loss: 0.509 | Val loss: 0.823 | Gen: eway aiway onditingway isway orkingsway\n",
            "Epoch:  12 | Train loss: 0.445 | Val loss: 0.779 | Gen: eway airway onditioningway isway orkingway\n",
            "Epoch:  13 | Train loss: 0.430 | Val loss: 0.618 | Gen: eway airway onditioniningcay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.418 | Val loss: 0.659 | Gen: eway airay ondititioningway isway orkingway\n",
            "Epoch:  15 | Train loss: 0.334 | Val loss: 0.591 | Gen: esthay airway ondititingcay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.288 | Val loss: 0.484 | Gen: ehthay airway onditiongcay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.281 | Val loss: 0.838 | Gen: elay airway onditioningway isway orgiway\n",
            "Epoch:  18 | Train loss: 0.330 | Val loss: 0.482 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.201 | Val loss: 0.402 | Gen: eway airway onditioningcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.153 | Val loss: 0.411 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.132 | Val loss: 0.383 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.127 | Val loss: 0.457 | Gen: eway airway onditioningncay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.116 | Val loss: 0.412 | Gen: eray airway onditioningcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.202 | Val loss: 0.420 | Gen: eray airway onditioningcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.127 | Val loss: 0.645 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.195 | Val loss: 0.474 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.157 | Val loss: 0.492 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.176 | Val loss: 0.492 | Gen: ehthay airway onditionioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.115 | Val loss: 0.263 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.072 | Val loss: 0.229 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.063 | Val loss: 0.333 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.060 | Val loss: 0.428 | Gen: ehthay airway onditioningncay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.060 | Val loss: 0.519 | Gen: ethay airway onditionioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.080 | Val loss: 0.277 | Gen: eray airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.131 | Val loss: 0.324 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.077 | Val loss: 0.448 | Gen: eray airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.128 | Val loss: 0.303 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.072 | Val loss: 0.351 | Gen: etay airway onditioningingcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.068 | Val loss: 0.297 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.055 | Val loss: 0.285 | Gen: etay airway onditioningingcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.039 | Val loss: 0.205 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.028 | Val loss: 0.203 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.022 | Val loss: 0.178 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.018 | Val loss: 0.174 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.015 | Val loss: 0.171 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.014 | Val loss: 0.165 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.012 | Val loss: 0.161 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.011 | Val loss: 0.162 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.014 | Val loss: 0.171 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.012 | Val loss: 0.172 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.049 | Val loss: 0.584 | Gen: ethay airway onditioningnay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.235 | Val loss: 0.566 | Gen: eway airway ondititioningnay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.127 | Val loss: 0.200 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.051 | Val loss: 0.251 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.051 | Val loss: 0.305 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.034 | Val loss: 0.151 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.019 | Val loss: 0.145 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.018 | Val loss: 0.144 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.014 | Val loss: 0.147 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.011 | Val loss: 0.138 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.009 | Val loss: 0.134 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.008 | Val loss: 0.136 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.007 | Val loss: 0.136 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.006 | Val loss: 0.135 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.006 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.005 | Val loss: 0.140 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.005 | Val loss: 0.140 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.005 | Val loss: 0.142 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.004 | Val loss: 0.144 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.004 | Val loss: 0.145 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.004 | Val loss: 0.148 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.003 | Val loss: 0.149 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.010 | Val loss: 0.391 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  74 | Train loss: 0.158 | Val loss: 0.473 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.077 | Val loss: 0.208 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.078 | Val loss: 0.476 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.052 | Val loss: 0.210 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.021 | Val loss: 0.138 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.011 | Val loss: 0.127 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.008 | Val loss: 0.123 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.007 | Val loss: 0.120 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.006 | Val loss: 0.119 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.005 | Val loss: 0.118 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.005 | Val loss: 0.117 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.004 | Val loss: 0.117 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.004 | Val loss: 0.117 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.004 | Val loss: 0.116 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.003 | Val loss: 0.116 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.003 | Val loss: 0.115 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.003 | Val loss: 0.116 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.003 | Val loss: 0.115 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.002 | Val loss: 0.116 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.002 | Val loss: 0.114 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.002 | Val loss: 0.123 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.002 | Val loss: 0.118 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.144 | Val loss: 2.611 | Gen: eray airway ondooclay isway orkway\n",
            "Epoch:  97 | Train loss: 0.382 | Val loss: 0.276 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.048 | Val loss: 0.203 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.021 | Val loss: 0.174 | Gen: ethay airway onditioningcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "141ArQpHfkiw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3fc4d930-472b-4cd0-a3fe-36e02bcc8259"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-OnIrHzjtKW",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAGAqKo2jpqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d28c8f12-b6fe-497d-cba7-03f815c3a39e"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "              'seed': 20200212\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "                                   seed: 20200212                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('utterance', 'utteranceway')\n",
            "('poor', 'oorpay')\n",
            "('charm', 'armchay')\n",
            "('constitutional', 'onstitutionalcay')\n",
            "('promised', 'omisedpray')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Epoch:   0 | Train loss: 2.347 | Val loss: 2.154 | Gen: etay ay ongngggay ililililililisisisis ongnggngggggrngay\n",
            "Epoch:   1 | Train loss: 1.619 | Val loss: 1.795 | Gen: tetay-ay aiwaray oingongwingwingwingw iiiway oowangray\n",
            "Epoch:   2 | Train loss: 1.371 | Val loss: 1.690 | Gen: tetay ayway oioningingigngngway- isissssssssssway ouongringngngway\n",
            "Epoch:   3 | Train loss: 1.212 | Val loss: 1.426 | Gen: tehththeway airway oinconningingway isisay offay\n",
            "Epoch:   4 | Train loss: 1.013 | Val loss: 1.258 | Gen: ehthhthehay airway oincondingay isay ornucoray\n",
            "Epoch:   5 | Train loss: 0.862 | Val loss: 1.275 | Gen: ehth--- airway oniontiongngay isway orngay\n",
            "Epoch:   6 | Train loss: 0.794 | Val loss: 1.256 | Gen: ehthhthay airway onditiondindincontin ississssway orknorknay\n",
            "Epoch:   7 | Train loss: 0.712 | Val loss: 1.173 | Gen: ehthay airway onitititititioncngwa isisay owurckiknay\n",
            "Epoch:   8 | Train loss: 0.688 | Val loss: 1.092 | Gen: ethay- airway ondioninctingwaypay isway orwikngwgway\n",
            "Epoch:   9 | Train loss: 0.619 | Val loss: 1.226 | Gen: -ot-othephay airwaiday ondinindininincay isisday orcornkay\n",
            "Epoch:  10 | Train loss: 0.595 | Val loss: 1.128 | Gen: ethhay airwairway oncondingway isway orikngway\n",
            "Epoch:  11 | Train loss: 0.529 | Val loss: 0.742 | Gen: ethay airwaiway onditiningcay isway orngway\n",
            "Epoch:  12 | Train loss: 0.440 | Val loss: 0.734 | Gen: ethay airwairway onditingcaypgray isay-isday orngway\n",
            "Epoch:  13 | Train loss: 0.379 | Val loss: 0.699 | Gen: eththeway airway onditingcay issway oruckikngway\n",
            "Epoch:  14 | Train loss: 0.335 | Val loss: 0.704 | Gen: ethethay airway onditingcay isway orkikngway\n",
            "Epoch:  15 | Train loss: 0.305 | Val loss: 0.627 | Gen: ethay airway onditingcay isway orkikngway\n",
            "Epoch:  16 | Train loss: 0.291 | Val loss: 0.612 | Gen: ethetway airway ondingcay isway orkikewngway\n",
            "Epoch:  17 | Train loss: 0.289 | Val loss: 0.850 | Gen: ethetay airwairway onditingcay isway orwikikay\n",
            "Epoch:  18 | Train loss: 0.326 | Val loss: 0.583 | Gen: ethetway airway ondingcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.258 | Val loss: 0.682 | Gen: ethay airway ondingcay isway orfikikngway\n",
            "Epoch:  20 | Train loss: 0.243 | Val loss: 0.578 | Gen: etheptay irway onditingscay isway orikingway\n",
            "Epoch:  21 | Train loss: 0.213 | Val loss: 0.563 | Gen: ethay airway ondingcay isway orwingway\n",
            "Epoch:  22 | Train loss: 0.224 | Val loss: 0.593 | Gen: ethetay airway onditinglay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.238 | Val loss: 0.747 | Gen: ethay airway onditingcay issway orkikikngway\n",
            "Epoch:  24 | Train loss: 0.212 | Val loss: 0.504 | Gen: ethetay airway ondingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.182 | Val loss: 0.484 | Gen: ethetay airway ondingcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.175 | Val loss: 0.476 | Gen: ethethay airway ondititingcay isway orkikingway\n",
            "Epoch:  27 | Train loss: 0.153 | Val loss: 0.438 | Gen: etthay airway ondititingcay isway orkikingway\n",
            "Epoch:  28 | Train loss: 0.137 | Val loss: 0.443 | Gen: ethay airway onditingcay isway orkringway\n",
            "Epoch:  29 | Train loss: 0.152 | Val loss: 0.523 | Gen: ethay airwrway ondititingcay isway orkikingway\n",
            "Epoch:  30 | Train loss: 0.207 | Val loss: 0.668 | Gen: ethay ayoirw ondititingcay isay orikingway\n",
            "Epoch:  31 | Train loss: 0.238 | Val loss: 0.750 | Gen: etay airway onditingcay isway ororfingway\n",
            "Epoch:  32 | Train loss: 0.179 | Val loss: 0.386 | Gen: ethay airway ondititingcay isway orkikingway\n",
            "Epoch:  33 | Train loss: 0.107 | Val loss: 0.382 | Gen: etthay airway ondititingcay isway orkikingway\n",
            "Epoch:  34 | Train loss: 0.092 | Val loss: 0.315 | Gen: ethay airway ondititingcay isway orkikingway\n",
            "Epoch:  35 | Train loss: 0.083 | Val loss: 0.325 | Gen: ethay airway ondititingsway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.077 | Val loss: 0.314 | Gen: ethay airway ondititingcay isway orkikingway\n",
            "Epoch:  37 | Train loss: 0.078 | Val loss: 0.306 | Gen: ethay airway ondititingcay isway orkikingway\n",
            "Epoch:  38 | Train loss: 0.079 | Val loss: 0.322 | Gen: ethay airway ondititingcay isway orkikingway\n",
            "Epoch:  39 | Train loss: 0.073 | Val loss: 0.367 | Gen: ethay airway ondititinglay isway orkikingway\n",
            "Epoch:  40 | Train loss: 0.078 | Val loss: 0.512 | Gen: ethay airway ondititioningway isway orkikingway\n",
            "Epoch:  41 | Train loss: 0.115 | Val loss: 0.871 | Gen: ethay airway onditionglingcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.167 | Val loss: 0.658 | Gen: etay airway ondititinglingcray isway orkikikingway\n",
            "Epoch:  43 | Train loss: 0.188 | Val loss: 0.604 | Gen: ethay ay-irara ondititinglay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.153 | Val loss: 0.386 | Gen: ethay airway onditintingcay isway orkikinbway\n",
            "Epoch:  45 | Train loss: 0.120 | Val loss: 0.337 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.087 | Val loss: 0.294 | Gen: ethay airway ondititiongcay isway orkikingway\n",
            "Epoch:  47 | Train loss: 0.067 | Val loss: 0.233 | Gen: ethay airway onditingcay isway orkikingway\n",
            "Epoch:  48 | Train loss: 0.061 | Val loss: 0.363 | Gen: ethay airway ondititininingsway isway orkingway\n",
            "Epoch:  49 | Train loss: 0.066 | Val loss: 0.266 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.046 | Val loss: 0.199 | Gen: etthay airway ondititiongcay isway orkikingway\n",
            "Epoch:  51 | Train loss: 0.043 | Val loss: 0.298 | Gen: ethay airway ondititiongway isway orkikingway\n",
            "Epoch:  52 | Train loss: 0.041 | Val loss: 0.221 | Gen: ethay airway ondititiongcay isway orkikingway\n",
            "Epoch:  53 | Train loss: 0.031 | Val loss: 0.223 | Gen: etthethay airway ondititiongcay isway orkikingway\n",
            "Epoch:  54 | Train loss: 0.034 | Val loss: 0.283 | Gen: etthay airway ondititiongway isway orkikingway\n",
            "Epoch:  55 | Train loss: 0.057 | Val loss: 0.442 | Gen: etthethay airway onditiniongcay isway orkikingway\n",
            "Epoch:  56 | Train loss: 0.093 | Val loss: 0.441 | Gen: ethay airway ondititingnglingscay isway orkikiringngsway\n",
            "Epoch:  57 | Train loss: 0.074 | Val loss: 0.335 | Gen: ethay airway onditingcay isway orkikingway\n",
            "Epoch:  58 | Train loss: 0.068 | Val loss: 0.414 | Gen: etthay airway ondititingcay isway orkikingway\n",
            "Epoch:  59 | Train loss: 0.135 | Val loss: 0.574 | Gen: etthay airwrway ondititingcay isway orkoringway\n",
            "Epoch:  60 | Train loss: 0.138 | Val loss: 0.647 | Gen: ethethay airwayay onditingcaygway isway orkingway\n",
            "Epoch:  61 | Train loss: 0.209 | Val loss: 0.689 | Gen: etay airway onditingwingway isway orkingway\n",
            "Epoch:  62 | Train loss: 0.154 | Val loss: 0.299 | Gen: ethay airway ondititingscay isway orkikingway\n",
            "Epoch:  63 | Train loss: 0.064 | Val loss: 0.255 | Gen: ethay airway ondititingcay isway orkikingway\n",
            "Epoch:  64 | Train loss: 0.045 | Val loss: 0.225 | Gen: ethay airway ondititiningcay isway orkikingway\n",
            "Epoch:  65 | Train loss: 0.044 | Val loss: 0.198 | Gen: ethay airway onditingcay isway orkikingway\n",
            "Epoch:  66 | Train loss: 0.035 | Val loss: 0.181 | Gen: ethay airway ondititiongcay isway orkikingway\n",
            "Epoch:  67 | Train loss: 0.026 | Val loss: 0.158 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.021 | Val loss: 0.169 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.018 | Val loss: 0.153 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.016 | Val loss: 0.182 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.016 | Val loss: 0.162 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.014 | Val loss: 0.189 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.014 | Val loss: 0.178 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.013 | Val loss: 0.190 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.015 | Val loss: 0.292 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.079 | Val loss: 0.343 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.075 | Val loss: 0.453 | Gen: etthay airway onditingcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.079 | Val loss: 0.278 | Gen: ethay airway ondititiongcay isway orkikingway\n",
            "Epoch:  79 | Train loss: 0.064 | Val loss: 0.323 | Gen: ethay airway onditingcay isway orkikingway\n",
            "Epoch:  80 | Train loss: 0.053 | Val loss: 0.265 | Gen: ethay airway ondititiongcay isway orkikingway\n",
            "Epoch:  81 | Train loss: 0.036 | Val loss: 0.198 | Gen: ethay airway onditiongcay isway orkikingway\n",
            "Epoch:  82 | Train loss: 0.034 | Val loss: 0.237 | Gen: ethay airway ondititiongcay isway orkikingway\n",
            "Epoch:  83 | Train loss: 0.032 | Val loss: 0.332 | Gen: ethethay airway ondititiongcay isway orkikingway\n",
            "Epoch:  84 | Train loss: 0.026 | Val loss: 0.204 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.015 | Val loss: 0.169 | Gen: ethay airway onditingcay isway orkikingway\n",
            "Epoch:  86 | Train loss: 0.012 | Val loss: 0.196 | Gen: ethay airway onditiniongcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.012 | Val loss: 0.175 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.008 | Val loss: 0.193 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.008 | Val loss: 0.185 | Gen: etthay airway onditingcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.007 | Val loss: 0.195 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.008 | Val loss: 0.200 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.013 | Val loss: 0.238 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.015 | Val loss: 0.212 | Gen: etthay airway onditiningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.017 | Val loss: 0.278 | Gen: etthay airway onditioningcay isway orkikingway\n",
            "Epoch:  95 | Train loss: 0.067 | Val loss: 1.193 | Gen: ethay airway oonditininonineninin isway oorkingway\n",
            "Epoch:  96 | Train loss: 0.120 | Val loss: 0.367 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.058 | Val loss: 0.327 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.044 | Val loss: 0.259 | Gen: ethay airway ondititingcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.038 | Val loss: 0.203 | Gen: ethay airway onditingcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditingcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVLDOi8MjyiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "01108e41-a869-4457-df62-4addd6f563cc"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditingcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a5JV-qYtGIW",
        "colab_type": "text"
      },
      "source": [
        "# Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRpH4F-Fs-fM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_WORD_ATTN = 'street'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wWmEpDetNMQ",
        "colab_type": "text"
      },
      "source": [
        "## Visualize RNN attention map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FQv0-nftKbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "de33c0c3-a697-4006-9c0f-a18d1932e87d"
      },
      "source": [
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4gJiZmaZuICYmVkmLiBmZpbJ/weuvvGV+3iRxQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eetstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZcY9o69tWfc",
        "colab_type": "text"
      },
      "source": [
        "## Visualize transformer attention maps from all the transformer layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzqnecFWtUrs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "0d1f559f-910f-4100-fbe9-54ee680c4b0a"
      },
      "source": [
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEhCAYAAABcN4ZbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM8vKhcXMzLJyYTEzs6xcWMzMLKv/D8yON+gABS1YAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKob1ifkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlsn/ABRLzDBbUuYeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eetstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM6StHz3tckO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}