{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w307LxNWi_on",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "import scipy\n",
        "#import scipy.misc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQIKYiXejQKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "def to_data(x):\n",
        "    \"\"\"Converts variable to numpy.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cpu()\n",
        "    return x.data.numpy()\n",
        "\n",
        "def create_dir(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "def gan_checkpoint(iteration, G, D, opts):\n",
        "    \"\"\"Saves the parameters of the generator G and discriminator D.\n",
        "    \"\"\"\n",
        "    G_path = os.path.join(opts.checkpoint_dir, 'G.pkl')\n",
        "    D_path = os.path.join(opts.checkpoint_dir, 'D.pkl')\n",
        "    torch.save(G.state_dict(), G_path)\n",
        "    torch.save(D.state_dict(), D_path)\n",
        "\n",
        "def cyclegan_checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, opts):\n",
        "    \"\"\"Saves the parameters of both generators G_YtoX, G_XtoY and discriminators D_X, D_Y.\n",
        "    \"\"\"\n",
        "    G_XtoY_path = os.path.join(opts.checkpoint_dir, 'G_XtoY.pkl')\n",
        "    G_YtoX_path = os.path.join(opts.checkpoint_dir, 'G_YtoX.pkl')\n",
        "    D_X_path = os.path.join(opts.checkpoint_dir, 'D_X.pkl')\n",
        "    D_Y_path = os.path.join(opts.checkpoint_dir, 'D_Y.pkl')\n",
        "    torch.save(G_XtoY.state_dict(), G_XtoY_path)\n",
        "    torch.save(G_YtoX.state_dict(), G_YtoX_path)\n",
        "    torch.save(D_X.state_dict(), D_X_path)\n",
        "    torch.save(D_Y.state_dict(), D_Y_path)\n",
        "\n",
        "\n",
        "def load_checkpoint(opts):\n",
        "    \"\"\"Loads the generator and discriminator models from checkpoints.\n",
        "    \"\"\"\n",
        "    G_XtoY_path = os.path.join(opts.load, 'G_XtoY.pkl')\n",
        "    G_YtoX_path = os.path.join(opts.load, 'G_YtoX.pkl')\n",
        "    D_X_path = os.path.join(opts.load, 'D_X.pkl')\n",
        "    D_Y_path = os.path.join(opts.load, 'D_Y.pkl')\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and opts.gpu) else \"cpu\")\n",
        "\n",
        "    G_XtoY = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights).to(device)\n",
        "    G_YtoX = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights).to(device)\n",
        "    D_X = DCDiscriminator(conv_dim=opts.d_conv_dim).to(device)\n",
        "    D_Y = DCDiscriminator(conv_dim=opts.d_conv_dim).to(device)\n",
        "\n",
        "    G_XtoY.load_state_dict(torch.load(G_XtoY_path, map_location=lambda storage, loc: storage))\n",
        "    G_YtoX.load_state_dict(torch.load(G_YtoX_path, map_location=lambda storage, loc: storage))\n",
        "    D_X.load_state_dict(torch.load(D_X_path, map_location=lambda storage, loc: storage))\n",
        "    D_Y.load_state_dict(torch.load(D_Y_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "    return G_XtoY, G_YtoX, D_X, D_Y\n",
        "\n",
        "\n",
        "def merge_images(sources, targets, opts):\n",
        "    \"\"\"Creates a grid consisting of pairs of columns, where the first column in\n",
        "    each pair contains images source images and the second column in each pair\n",
        "    contains images generated by the CycleGAN from the corresponding images in\n",
        "    the first column.\n",
        "    \"\"\"\n",
        "    _, _, h, w = sources.shape\n",
        "    row = int(np.sqrt(opts.batch_size))\n",
        "    merged = np.zeros([3, row*h, row*w*2])\n",
        "    for (idx, s, t) in (zip(range(row**2), sources, targets,)):\n",
        "        i = idx // row\n",
        "        j = idx % row\n",
        "        merged[:, i*h:(i+1)*h, (j*2)*h:(j*2+1)*h] = s\n",
        "        merged[:, i*h:(i+1)*h, (j*2+1)*h:(j*2+2)*h] = t\n",
        "    return merged.transpose(1, 2, 0)\n",
        "\n",
        "def create_image_grid(array, ncols=None):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    num_images, channels, cell_h, cell_w = array.shape\n",
        "    if not ncols:\n",
        "        ncols = int(np.sqrt(num_images))\n",
        "    nrows = int(np.math.floor(num_images / float(ncols)))\n",
        "    result = np.zeros((cell_h*nrows, cell_w*ncols, channels), dtype=array.dtype)\n",
        "    for i in range(0, nrows):\n",
        "        for j in range(0, ncols):\n",
        "            result[i*cell_h:(i+1)*cell_h, j*cell_w:(j+1)*cell_w, :] = array[i*ncols+j].transpose(1, 2, 0)\n",
        "\n",
        "    if channels == 1:\n",
        "        result = result.squeeze()\n",
        "    return result\n",
        "  \n",
        "def gan_save_samples(G, fixed_noise, iteration, opts):\n",
        "    generated_images = G(fixed_noise)\n",
        "    generated_images = vutils.make_grid(generated_images, padding=2, normalize=True)\n",
        "    grid = np.transpose(to_data(generated_images),(1,2,0))\n",
        "\n",
        "#    grid = create_image_grid(generated_images)\n",
        "\n",
        "    # merged = merge_images(X, fake_Y, opts)\n",
        "    path = os.path.join(opts.sample_dir, 'sample-{:06d}.png'.format(iteration))\n",
        "#    print(grid)\n",
        "    plt.imsave(path, grid)\n",
        "#    scipy.misc.imsave(path, grid)\n",
        "    print('Saved {}'.format(path))\n",
        "\n",
        "def cyclegan_save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts):\n",
        "    \"\"\"Saves samples from both generators X->Y and Y->X.\n",
        "    \"\"\"\n",
        "    fake_X = G_YtoX(fixed_Y)\n",
        "    fake_Y = G_XtoY(fixed_X)\n",
        "\n",
        "    X, fake_X = to_data(fixed_X), to_data(fake_X)\n",
        "    Y, fake_Y = to_data(fixed_Y), to_data(fake_Y)\n",
        "\n",
        "    merged = merge_images(X, fake_Y, opts)\n",
        "    path = os.path.join(opts.sample_dir, 'sample-{:06d}-X-Y.png'.format(iteration))\n",
        "#    plt.imsave(path, merged)\n",
        "#    scipy.misc.imsave(path, merged)\n",
        "    print('Saved {}'.format(path))\n",
        "\n",
        "    merged = merge_images(Y, fake_X, opts)\n",
        "    path = os.path.join(opts.sample_dir, 'sample-{:06d}-Y-X.png'.format(iteration))\n",
        "#    plt.imsave(path, merged)\n",
        " #   scipy.misc.imsave(path, merged)\n",
        "    print('Saved {}'.format(path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHoj1l-clQ0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_emoji_loader(emoji_type, opts):\n",
        "    \"\"\"Creates training and test data loaders.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "                    transforms.Resize(opts.image_size),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                ])\n",
        "\n",
        "    train_path = os.path.join('data/emojis', emoji_type)\n",
        "    test_path = os.path.join('data/emojis', 'Test_{}'.format(emoji_type))\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
        "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
        "\n",
        "    train_dloader = DataLoader(dataset=train_dataset, batch_size=opts.batch_size, shuffle=True, num_workers=opts.num_workers)\n",
        "    test_dloader = DataLoader(dataset=test_dataset, batch_size=opts.batch_size, shuffle=False, num_workers=opts.num_workers)\n",
        "\n",
        "    return train_dloader, test_dloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBVC3ABplgeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_models(G_XtoY, G_YtoX, D_X, D_Y):\n",
        "    \"\"\"Prints model information for the generators and discriminators.\n",
        "    \"\"\"\n",
        "    if G_YtoX:\n",
        "      print(\"                 G_XtoY                \")\n",
        "      print(\"---------------------------------------\")\n",
        "      print(G_XtoY)\n",
        "      print(\"---------------------------------------\")\n",
        "\n",
        "      print(\"                 G_YtoX                \")\n",
        "      print(\"---------------------------------------\")\n",
        "      print(G_YtoX)\n",
        "      print(\"---------------------------------------\")\n",
        "\n",
        "      print(\"                  D_X                  \")\n",
        "      print(\"---------------------------------------\")\n",
        "      print(D_X)\n",
        "      print(\"---------------------------------------\")\n",
        "\n",
        "      print(\"                  D_Y                  \")\n",
        "      print(\"---------------------------------------\")\n",
        "      print(D_Y)\n",
        "      print(\"---------------------------------------\")\n",
        "    else:\n",
        "      print(\"                 G                     \")\n",
        "      print(\"---------------------------------------\")\n",
        "      print(G_XtoY)\n",
        "      print(\"---------------------------------------\")\n",
        "\n",
        "      print(\"                  D                    \")\n",
        "      print(\"---------------------------------------\")\n",
        "      print(D_X)\n",
        "      print(\"---------------------------------------\")\n",
        "\n",
        "      \n",
        "def create_model(opts):\n",
        "    \"\"\"Builds the generators and discriminators.\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and opts.gpu) else \"cpu\")\n",
        "\n",
        "    if opts.Y is None:\n",
        "      ### GAN\n",
        "      G = DCGenerator(noise_size=opts.noise_size, conv_dim=opts.g_conv_dim).to(device)\n",
        "      D = DCDiscriminator(conv_dim=opts.d_conv_dim).to(device)\n",
        "\n",
        "      print_models(G, None, D, None)\n",
        "\n",
        "      return G, D\n",
        "          \n",
        "    else:\n",
        "      ### CycleGAN\n",
        "      G_XtoY = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights).to(device)\n",
        "      G_YtoX = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights).to(device)\n",
        "      D_X = DCDiscriminator(conv_dim=opts.d_conv_dim).to(device)\n",
        "      D_Y = DCDiscriminator(conv_dim=opts.d_conv_dim).to(device)\n",
        "\n",
        "      print_models(G_XtoY, G_YtoX, D_X, D_Y)\n",
        "\n",
        "      return G_XtoY, G_YtoX, D_X, D_Y\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    \"\"\"Loads the data, creates checkpoint and sample directories, and starts the training loop.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create train and test dataloaders for images from the two domains X and Y\n",
        "    dataloader_X, test_dataloader_X = get_emoji_loader(emoji_type=opts.X, opts=opts)\n",
        "    if opts.Y:\n",
        "      dataloader_Y, test_dataloader_Y = get_emoji_loader(emoji_type=opts.Y, opts=opts)\n",
        "\n",
        "    # Create checkpoint and sample directories\n",
        "    create_dir(opts.checkpoint_dir)\n",
        "    create_dir(opts.sample_dir)\n",
        "\n",
        "    # Start training\n",
        "    if opts.Y is None:\n",
        "      G, D = gan_training_loop(dataloader_X, test_dataloader_X, opts)\n",
        "      return G, D\n",
        "    else:\n",
        "      G_XtoY, G_YtoX, D_X, D_Y = cyclegan_training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, opts)\n",
        "      return G_XtoY, G_YtoX, D_X, D_Y\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        if opts.__dict__[key]:\n",
        "            print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWsjd7VKmCB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_noise(batch_size, dim):\n",
        "    \"\"\"\n",
        "    Generate a PyTorch Tensor of uniform random noise.\n",
        "\n",
        "    Input:\n",
        "    - batch_size: Integer giving the batch size of noise to generate.\n",
        "    - dim: Integer giving the dimension of noise to generate.\n",
        "\n",
        "    Output:\n",
        "    - A PyTorch Tensor of shape (batch_size, dim, 1, 1) containing uniform\n",
        "      random noise in the range (-1, 1).\n",
        "    \"\"\"\n",
        "    return (torch.rand(batch_size, dim) * 2 - 1).unsqueeze(2).unsqueeze(3)\n",
        "  \n",
        "\n",
        "def upconv(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True):\n",
        "    \"\"\"Creates a upsample-and-convolution layer, with optional batch normalization.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    if stride>1:\n",
        "      layers.append(nn.Upsample(scale_factor=stride))\n",
        "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)\n",
        "    layers.append(conv_layer)\n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True, init_zero_weights=False):\n",
        "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "    if init_zero_weights:\n",
        "        conv_layer.weight.data = torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.001\n",
        "    layers.append(conv_layer)\n",
        "\n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "  \n",
        "  \n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, conv_dim):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_layer = conv(in_channels=conv_dim, out_channels=conv_dim, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_layer(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQFI_kjCm86d",
        "colab_type": "text"
      },
      "source": [
        "# **DCGAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdfM2MXanGAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCGenerator(nn.Module):\n",
        "    def __init__(self, noise_size, conv_dim):\n",
        "        super(DCGenerator, self).__init__()\n",
        "\n",
        "        self.conv_dim = conv_dim\n",
        "        ###########################################\n",
        "        ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n",
        "        ###########################################\n",
        "\n",
        "        self.linear_bn = upconv(noise_size, 128, 5, 4, 2, True)\n",
        "        self.upconv1 = upconv(128, 64, 5, 2, 2, True)\n",
        "        self.upconv2 = upconv(64, 32, 5, 2, 2, True)\n",
        "        self.upconv3 = upconv(32, 3, 5, 2, 2, False)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"Generates an image given a sample of random noise.\n",
        "\n",
        "            Input\n",
        "            -----\n",
        "                z: BS x noise_size x 1 x 1   -->  BSx100x1x1 (during training)\n",
        "\n",
        "            Output\n",
        "            ------\n",
        "                out: BS x channels x image_width x image_height  -->  BSx3x32x32 (during training)\n",
        "        \"\"\"\n",
        "        batch_size = z.size(0)\n",
        "        \n",
        "        out = F.relu(self.linear_bn(z)).view(-1, self.conv_dim*4, 4, 4)    # BS x 128 x 4 x 4\n",
        "        out = F.relu(self.upconv1(out))  # BS x 64 x 8 x 8\n",
        "        out = F.relu(self.upconv2(out))  # BS x 32 x 16 x 16\n",
        "        out = torch.tanh(self.upconv3(out))  # BS x 3 x 32 x 32\n",
        "        \n",
        "        out_size = out.size()\n",
        "        if out_size != torch.Size([batch_size, 3, 32, 32]):\n",
        "          raise ValueError(\"expect {} x 3 x 32 x 32, but get {}\".format(batch_size, out_size))\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0MVHoaDnUOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCDiscriminator(nn.Module):\n",
        "    \"\"\"Defines the architecture of the discriminator network.\n",
        "       Note: Both discriminators D_X and D_Y have the same architecture in this assignment.\n",
        "    \"\"\"\n",
        "    def __init__(self, conv_dim=64):\n",
        "        super(DCDiscriminator, self).__init__()\n",
        "\n",
        "        ###########################################\n",
        "        ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n",
        "        ###########################################\n",
        "\n",
        "        self.conv1 = conv(3, 32, 5, stride=2, padding=2, batch_norm=True, init_zero_weights=True)\n",
        "        self.conv2 = conv(32, 64, 5, stride=2, padding=2, batch_norm=True, init_zero_weights=True)\n",
        "        self.conv3 = conv(64, 128, 5, stride=2, padding=2, batch_norm=True, init_zero_weights=True)\n",
        "        self.conv4 = conv(128, 1, 4, stride=1, padding=0, batch_norm=False, init_zero_weights=True)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        out = F.relu(self.conv1(x))    # BS x 64 x 16 x 16\n",
        "        out = F.relu(self.conv2(out))    # BS x 64 x 8 x 8\n",
        "        out = F.relu(self.conv3(out))    # BS x 64 x 4 x 4\n",
        "\n",
        "        out = self.conv4(out).squeeze()\n",
        "        out_size = out.size()\n",
        "        if out_size != torch.Size([batch_size,]):\n",
        "          raise ValueError(\"expect {} x 1, but get {}\".format(batch_size, out_size))\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hqw5HSLxF3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_label = 1\n",
        "fake_label = 0\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTlrHO4ynYBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gan_training_loop(dataloader, test_dataloader, opts):\n",
        "    \"\"\"Runs the training loop.\n",
        "        * Saves checkpoint every opts.checkpoint_every iterations\n",
        "        * Saves generated samples every opts.sample_every iterations\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and opts.gpu) else \"cpu\")\n",
        "\n",
        "    # Create generators and discriminators\n",
        "    G, D = create_model(opts)\n",
        "\n",
        "    g_params = G.parameters()  # Get generator parameters\n",
        "    d_params = D.parameters()  # Get discriminator parameters\n",
        "\n",
        "    # Create optimizers for the generators and discriminators\n",
        "    g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n",
        "    d_optimizer = optim.Adam(d_params, opts.lr*2., [opts.beta1, opts.beta2])\n",
        "\n",
        "    train_iter = iter(dataloader)\n",
        "\n",
        "    test_iter = iter(test_dataloader)\n",
        "\n",
        "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
        "    # constant throughout training, that allow us to inspect the model's performance.\n",
        "    fixed_noise = sample_noise(100, opts.noise_size).to(device)  # # 100 x noise_size x 1 x 1\n",
        "\n",
        "    iter_per_epoch = len(train_iter)\n",
        "    total_train_iters = opts.train_iters\n",
        "\n",
        "    try:\n",
        "      for iteration in range(1, opts.train_iters+1):\n",
        "\n",
        "          # Reset data_iter for each epoch\n",
        "          if iteration % iter_per_epoch == 0:\n",
        "              train_iter = iter(dataloader)\n",
        "\n",
        "          real_images, real_labels = train_iter.next()\n",
        "          real_images, real_labels = real_images.to(device), real_labels.long().squeeze().to(device)\n",
        "\n",
        "          d_optimizer.zero_grad()\n",
        "\n",
        "          # FILL THIS IN\n",
        "          # 1. Compute the discriminator loss on real images\n",
        "          b_size = real_images.size(0)\n",
        "          label = torch.full((b_size,), real_label, device=device)\n",
        "          D_real_loss = criterion(D(real_images), label) / 2\n",
        "\n",
        "          # 2. Sample noise\n",
        "          noise = sample_noise(b_size, opts.noise_size).to(device)\n",
        "\n",
        "          # 3. Generate fake images from the noise\n",
        "          fake_images = G(noise)\n",
        "\n",
        "          # 4. Compute the discriminator loss on the fake images\n",
        "          label = torch.full((b_size,), fake_label, device=device)\n",
        "          D_fake_loss = criterion(D(fake_images), label) / 2       \n",
        "\n",
        "          # 5. Compute the total discriminator loss\n",
        "          D_total_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "          D_total_loss.backward()\n",
        "          d_optimizer.step()\n",
        "\n",
        "          ###########################################\n",
        "          ###          TRAIN THE GENERATOR        ###\n",
        "          ###########################################\n",
        "\n",
        "          g_optimizer.zero_grad()\n",
        "\n",
        "          # FILL THIS IN\n",
        "          # 1. Sample noise\n",
        "          noise = sample_noise(b_size, opts.noise_size).to(device)\n",
        "\n",
        "          # 2. Generate fake images from the noise\n",
        "          fake_images = G(noise)\n",
        "\n",
        "          # 3. Compute the generator loss\n",
        "          label = torch.full((b_size,), real_label, device=device)\n",
        "          G_loss = criterion(D(fake_images), label)\n",
        "          \n",
        "          G_loss.backward()\n",
        "          g_optimizer.step()\n",
        "\n",
        "\n",
        "          # Print the log info\n",
        "          if iteration % opts.log_step == 0:\n",
        "\n",
        "              print('Iteration [{:4d}/{:4d}] | D_real_loss: {:6.4f} | D_fake_loss: {:6.4f} | G_loss: {:6.4f}'.format(\n",
        "                     iteration, total_train_iters, D_real_loss.item(), D_fake_loss.item(), G_loss.item()))\n",
        "\n",
        "          # Save the generated samples\n",
        "          if iteration % opts.sample_every == 0:\n",
        "            with torch.no_grad():\n",
        "              gan_save_samples(G, fixed_noise, iteration, opts)\n",
        "\n",
        "          # Save the model parameters\n",
        "          if iteration % opts.checkpoint_every == 0:\n",
        "              gan_checkpoint(iteration, G, D, opts)\n",
        "              \n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return G, D\n",
        "      \n",
        "    return G, D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75TehkdE0gzG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fd0f814-ad79-4f26-cdec-60a528ffe715"
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='emojis', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/emojis.tar.gz', \n",
        "                         untar=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/emojis.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU7zDlzU0lJa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9ef2051-44c0-4ec2-ae7a-116783960fa0"
      },
      "source": [
        "SEED = 11011\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'image_size':32, \n",
        "              'g_conv_dim':32, \n",
        "              'd_conv_dim':64,\n",
        "              'noise_size':100,\n",
        "              'num_workers': 0,\n",
        "              'train_iters':5000,\n",
        "              'X':'Windows',  # options: 'Windows' / 'Apple'\n",
        "              'Y': None,\n",
        "              'lr':0.0003,\n",
        "              'beta1':0.5,\n",
        "              'beta2':0.999,\n",
        "              'batch_size':32, \n",
        "              'checkpoint_dir': 'checkpoints_gan',\n",
        "              'sample_dir': 'samples_gan',\n",
        "              'load': None,\n",
        "              'log_step':200,\n",
        "              'sample_every':200,\n",
        "              'checkpoint_every':1000,\n",
        "              'gpu': True\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "G, D = train(args)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                             image_size: 32                                     \n",
            "                             g_conv_dim: 32                                     \n",
            "                             d_conv_dim: 64                                     \n",
            "                             noise_size: 100                                    \n",
            "                            train_iters: 5000                                   \n",
            "                                      X: Windows                                \n",
            "                                     lr: 0.0003                                 \n",
            "                                  beta1: 0.5                                    \n",
            "                                  beta2: 0.999                                  \n",
            "                             batch_size: 32                                     \n",
            "                         checkpoint_dir: checkpoints_gan                        \n",
            "                             sample_dir: samples_gan                            \n",
            "                               log_step: 200                                    \n",
            "                           sample_every: 200                                    \n",
            "                       checkpoint_every: 1000                                   \n",
            "                                    gpu: 1                                      \n",
            "================================================================================\n",
            "                 G                     \n",
            "---------------------------------------\n",
            "DCGenerator(\n",
            "  (linear_bn): Sequential(\n",
            "    (0): Upsample(scale_factor=4.0, mode=nearest)\n",
            "    (1): Conv2d(100, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (upconv1): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (1): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (upconv2): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (1): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (upconv3): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (1): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "  )\n",
            ")\n",
            "---------------------------------------\n",
            "                  D                    \n",
            "---------------------------------------\n",
            "DCDiscriminator(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "  )\n",
            ")\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration [ 200/5000] | D_real_loss: 0.0277 | D_fake_loss: 0.0103 | G_loss: 0.7754\n",
            "Saved samples_gan/sample-000200.png\n",
            "Iteration [ 400/5000] | D_real_loss: 0.0164 | D_fake_loss: 0.0269 | G_loss: 0.9543\n",
            "Saved samples_gan/sample-000400.png\n",
            "Iteration [ 600/5000] | D_real_loss: 0.0601 | D_fake_loss: 0.0070 | G_loss: 0.6843\n",
            "Saved samples_gan/sample-000600.png\n",
            "Iteration [ 800/5000] | D_real_loss: 0.0309 | D_fake_loss: 0.0055 | G_loss: 0.7231\n",
            "Saved samples_gan/sample-000800.png\n",
            "Iteration [1000/5000] | D_real_loss: 0.0053 | D_fake_loss: 0.0143 | G_loss: 1.0770\n",
            "Saved samples_gan/sample-001000.png\n",
            "Iteration [1200/5000] | D_real_loss: 0.0039 | D_fake_loss: 0.0288 | G_loss: 0.9603\n",
            "Saved samples_gan/sample-001200.png\n",
            "Iteration [1400/5000] | D_real_loss: 0.0076 | D_fake_loss: 0.0498 | G_loss: 1.2478\n",
            "Saved samples_gan/sample-001400.png\n",
            "Iteration [1600/5000] | D_real_loss: 0.0092 | D_fake_loss: 0.0194 | G_loss: 0.7364\n",
            "Saved samples_gan/sample-001600.png\n",
            "Iteration [1800/5000] | D_real_loss: 0.0060 | D_fake_loss: 0.0130 | G_loss: 1.1706\n",
            "Saved samples_gan/sample-001800.png\n",
            "Iteration [2000/5000] | D_real_loss: 0.0035 | D_fake_loss: 0.0134 | G_loss: 1.0868\n",
            "Saved samples_gan/sample-002000.png\n",
            "Iteration [2200/5000] | D_real_loss: 0.0055 | D_fake_loss: 0.0076 | G_loss: 0.9712\n",
            "Saved samples_gan/sample-002200.png\n",
            "Iteration [2400/5000] | D_real_loss: 0.0030 | D_fake_loss: 0.0131 | G_loss: 1.1025\n",
            "Saved samples_gan/sample-002400.png\n",
            "Iteration [2600/5000] | D_real_loss: 0.0045 | D_fake_loss: 0.0079 | G_loss: 1.2259\n",
            "Saved samples_gan/sample-002600.png\n",
            "Iteration [2800/5000] | D_real_loss: 0.0039 | D_fake_loss: 0.0065 | G_loss: 1.1528\n",
            "Saved samples_gan/sample-002800.png\n",
            "Iteration [3000/5000] | D_real_loss: 0.0056 | D_fake_loss: 0.0066 | G_loss: 1.2032\n",
            "Saved samples_gan/sample-003000.png\n",
            "Iteration [3200/5000] | D_real_loss: 0.0040 | D_fake_loss: 0.0061 | G_loss: 1.0263\n",
            "Saved samples_gan/sample-003200.png\n",
            "Iteration [3400/5000] | D_real_loss: 0.0021 | D_fake_loss: 0.0034 | G_loss: 1.2614\n",
            "Saved samples_gan/sample-003400.png\n",
            "Iteration [3600/5000] | D_real_loss: 0.0064 | D_fake_loss: 0.0020 | G_loss: 1.4221\n",
            "Saved samples_gan/sample-003600.png\n",
            "Iteration [3800/5000] | D_real_loss: 0.0214 | D_fake_loss: 0.0213 | G_loss: 1.3389\n",
            "Saved samples_gan/sample-003800.png\n",
            "Iteration [4000/5000] | D_real_loss: 0.0562 | D_fake_loss: 0.0169 | G_loss: 0.7056\n",
            "Saved samples_gan/sample-004000.png\n",
            "Iteration [4200/5000] | D_real_loss: 0.0050 | D_fake_loss: 0.0088 | G_loss: 1.0372\n",
            "Saved samples_gan/sample-004200.png\n",
            "Iteration [4400/5000] | D_real_loss: 0.0095 | D_fake_loss: 0.0029 | G_loss: 0.8689\n",
            "Saved samples_gan/sample-004400.png\n",
            "Iteration [4600/5000] | D_real_loss: 0.0304 | D_fake_loss: 0.0091 | G_loss: 0.5674\n",
            "Saved samples_gan/sample-004600.png\n",
            "Iteration [4800/5000] | D_real_loss: 0.0105 | D_fake_loss: 0.0072 | G_loss: 0.8364\n",
            "Saved samples_gan/sample-004800.png\n",
            "Iteration [5000/5000] | D_real_loss: 0.0021 | D_fake_loss: 0.0055 | G_loss: 0.9724\n",
            "Saved samples_gan/sample-005000.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUQ09e2HD64O",
        "colab_type": "text"
      },
      "source": [
        "### **CycleGAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwFl88m40vNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CycleGenerator(nn.Module):\n",
        "    \"\"\"Defines the architecture of the generator network.\n",
        "       Note: Both generators G_XtoY and G_YtoX have the same architecture in this assignment.\n",
        "    \"\"\"\n",
        "    def __init__(self, conv_dim=64, init_zero_weights=False):\n",
        "        super(CycleGenerator, self).__init__()\n",
        "\n",
        "        ###########################################\n",
        "        ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n",
        "        ###########################################\n",
        "\n",
        "        # 1. Define the encoder part of the generator (that extracts features from the input image)\n",
        "        self.conv1 = conv(3, 32, 5, stride=2, padding=2, batch_norm=True, init_zero_weights=init_zero_weights)\n",
        "        self.conv2 = conv(32, 64, 5, stride=2, padding=2, batch_norm=True, init_zero_weights=init_zero_weights)\n",
        "\n",
        "        # 2. Define the transformation part of the generator\n",
        "        self.resnet_block = ResnetBlock(64)\n",
        "\n",
        "        # 3. Define the decoder part of the generator (that builds up the output image from features)\n",
        "        self.upconv1 = upconv(64, 32, 5, 2, 2, True)\n",
        "        self.upconv2 = upconv(32, 3, 5, 2, 2, False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Generates an image conditioned on an input image.\n",
        "\n",
        "            Input\n",
        "            -----\n",
        "                x: BS x 3 x 32 x 32\n",
        "\n",
        "            Output\n",
        "            ------\n",
        "                out: BS x 3 x 32 x 32\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        out = F.relu(self.conv1(x))            # BS x 32 x 16 x 16\n",
        "        out = F.relu(self.conv2(out))          # BS x 64 x 8 x 8\n",
        "        \n",
        "        out = F.relu(self.resnet_block(out))   # BS x 64 x 8 x 8\n",
        "\n",
        "        out = F.relu(self.upconv1(out))        # BS x 32 x 16 x 16\n",
        "        out = torch.tanh(self.upconv2(out))        # BS x 3 x 32 x 32\n",
        "        \n",
        "        out_size = out.size()\n",
        "        if out_size != torch.Size([batch_size, 3, 32, 32]):\n",
        "          raise ValueError(\"expect {} x 3 x 32 x 32, but get {}\".format(batch_size, out_size))\n",
        "\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPy796ryLJsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "consistency_loss = nn.L1Loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h4EEiokGOZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cyclegan_training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, opts):\n",
        "    \"\"\"Runs the training loop.\n",
        "        * Saves checkpoint every opts.checkpoint_every iterations\n",
        "        * Saves generated samples every opts.sample_every iterations\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and opts.gpu) else \"cpu\")\n",
        "\n",
        "    # Create generators and discriminators\n",
        "    G_XtoY, G_YtoX, D_X, D_Y = create_model(opts)\n",
        "\n",
        "    g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters\n",
        "    d_params = list(D_X.parameters()) + list(D_Y.parameters())  # Get discriminator parameters\n",
        "\n",
        "    # Create optimizers for the generators and discriminators\n",
        "    g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n",
        "    d_optimizer = optim.Adam(d_params, opts.lr, [opts.beta1, opts.beta2])\n",
        "\n",
        "    iter_X = iter(dataloader_X)\n",
        "    iter_Y = iter(dataloader_Y)\n",
        "\n",
        "    test_iter_X = iter(test_dataloader_X)\n",
        "    test_iter_Y = iter(test_dataloader_Y)\n",
        "\n",
        "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
        "    # constant throughout training, that allow us to inspect the model's performance.\n",
        "    fixed_X = test_iter_X.next()[0].to(device)\n",
        "    fixed_Y = test_iter_Y.next()[0].to(device)\n",
        "\n",
        "    iter_per_epoch = min(len(iter_X), len(iter_Y))\n",
        "\n",
        "    try:\n",
        "      for iteration in range(1, opts.train_iters+1):\n",
        "\n",
        "          # Reset data_iter for each epoch\n",
        "          if iteration % iter_per_epoch == 0:\n",
        "              iter_X = iter(dataloader_X)\n",
        "              iter_Y = iter(dataloader_Y)\n",
        "\n",
        "          images_X, labels_X = iter_X.next()\n",
        "          images_X, labels_X = images_X.to(device), labels_X.long().squeeze().to(device)\n",
        "\n",
        "          images_Y, labels_Y = iter_Y.next()\n",
        "          images_Y, labels_Y = images_Y.to(device), labels_Y.long().squeeze().to(device)\n",
        "\n",
        "          bszx = images_X.size(0)\n",
        "          bszy = images_Y.size(0)\n",
        "\n",
        "          # ============================================\n",
        "          #            TRAIN THE DISCRIMINATORS\n",
        "          # ============================================\n",
        "\n",
        "          #########################################\n",
        "          ##             FILL THIS IN            ##\n",
        "          #########################################\n",
        "\n",
        "          # Train with real images\n",
        "          d_optimizer.zero_grad()\n",
        "\n",
        "          # 1. Compute the discriminator losses on real images\n",
        "          labelx = torch.full((bszx,), real_label, device=device)\n",
        "          labely = torch.full((bszy,), real_label, device=device)\n",
        "          D_X_loss = criterion(D_X(images_X), labelx)\n",
        "          D_Y_loss = criterion(D_Y(images_Y), labely)\n",
        "          \n",
        "          d_real_loss = D_X_loss + D_Y_loss\n",
        "          d_real_loss.backward()\n",
        "          d_optimizer.step()\n",
        "\n",
        "          # Train with fake images\n",
        "          d_optimizer.zero_grad()\n",
        "\n",
        "          # 2. Generate fake images that look like domain X based on real images in domain Y\n",
        "          fake_X = G_YtoX(images_Y)\n",
        "\n",
        "          # 3. Compute the loss for D_X\n",
        "          labelx = torch.full((bszy,), fake_label, device=device)\n",
        "          D_X_loss = criterion(D_X(fake_X), labelx)\n",
        "\n",
        "          # 4. Generate fake images that look like domain Y based on real images in domain X\n",
        "          fake_Y = G_XtoY(images_X)\n",
        "\n",
        "          # 5. Compute the loss for D_Y\n",
        "          labely = torch.full((bszx,), fake_label, device=device)\n",
        "          D_Y_loss = criterion(D_Y(fake_Y), labely)\n",
        "\n",
        "          d_fake_loss = D_X_loss + D_Y_loss\n",
        "          d_fake_loss.backward()\n",
        "          d_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "          # =========================================\n",
        "          #            TRAIN THE GENERATORS\n",
        "          # =========================================\n",
        "\n",
        "\n",
        "          #########################################\n",
        "          ##    FILL THIS IN: Y--X-->Y CYCLE     ##\n",
        "          #########################################\n",
        "          g_optimizer.zero_grad()\n",
        "\n",
        "          # 1. Generate fake images that look like domain X based on real images in domain Y\n",
        "          fake_X = G_YtoX(images_Y)\n",
        "\n",
        "          # 2. Compute the generator loss based on domain X\n",
        "          labelx = torch.full((bszy,), real_label, device=device)\n",
        "          g_loss = criterion(D_X(fake_X), labelx)\n",
        "\n",
        "          reconstructed_Y = G_XtoY(fake_X)\n",
        "          # 3. Compute the cycle consistency loss (the reconstruction loss)\n",
        "          cycle_consistency_loss = consistency_loss(images_Y, reconstructed_Y)\n",
        "          \n",
        "          g_loss += opts.lambda_cycle * cycle_consistency_loss\n",
        "          \n",
        "          g_loss.backward()\n",
        "          g_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "          #########################################\n",
        "          ##    FILL THIS IN: X--Y-->X CYCLE     ##\n",
        "          #########################################\n",
        "\n",
        "          g_optimizer.zero_grad()\n",
        "\n",
        "          # 1. Generate fake images that look like domain Y based on real images in domain X\n",
        "          fake_Y = G_XtoY(images_X)\n",
        "\n",
        "          # 2. Compute the generator loss based on domain Y\n",
        "          labely = torch.full((bszx,), real_label, device=device)\n",
        "          g_loss = criterion(D_Y(fake_Y), labely)\n",
        "\n",
        "          reconstructed_X = G_YtoX(fake_Y)\n",
        "          # 3. Compute the cycle consistency loss (the reconstruction loss)\n",
        "          cycle_consistency_loss = consistency_loss(images_X, reconstructed_X)\n",
        "          \n",
        "          g_loss += opts.lambda_cycle * cycle_consistency_loss\n",
        "\n",
        "          g_loss.backward()\n",
        "          g_optimizer.step()\n",
        "\n",
        "\n",
        "          # Print the log info\n",
        "          if iteration % opts.log_step == 0:\n",
        "              print('Iteration [{:5d}/{:5d}] | d_real_loss: {:6.4f} | d_Y_loss: {:6.4f} | d_X_loss: {:6.4f} | '\n",
        "                    'd_fake_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
        "                      iteration, opts.train_iters, d_real_loss.item(), D_Y_loss.item(),\n",
        "                      D_X_loss.item(), d_fake_loss.item(), g_loss.item()))\n",
        "\n",
        "\n",
        "          # Save the generated samples\n",
        "          if iteration % opts.sample_every == 0:\n",
        "              cyclegan_save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts)\n",
        "\n",
        "\n",
        "          # Save the model parameters\n",
        "          if iteration % opts.checkpoint_every == 0:\n",
        "              cyclegan_checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, opts)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return G_XtoY, G_YtoX, D_X, D_Y\n",
        "      \n",
        "    return G_XtoY, G_YtoX, D_X, D_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaPHE8EaL-gQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9643b6aa-2130-409d-9651-8e462046abab"
      },
      "source": [
        "SEED = 4302\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'image_size':32, \n",
        "              'g_conv_dim':32, \n",
        "              'd_conv_dim':32,\n",
        "              'init_zero_weights': False,\n",
        "              'num_workers': 0,\n",
        "              'train_iters':10000,\n",
        "              'X':'Apple',\n",
        "              'Y':'Windows',\n",
        "              'lambda_cycle': 0.015,\n",
        "              'lr':0.0003,\n",
        "              'beta1':0.3,\n",
        "              'beta2':0.999,\n",
        "              'batch_size':32, \n",
        "              'checkpoint_dir': 'checkpoints_cyclegan',\n",
        "              'sample_dir': 'samples_cyclegan',\n",
        "              'load': None,\n",
        "              'log_step':500,\n",
        "              'sample_every':500,\n",
        "              'checkpoint_every':1000,\n",
        "              'gpu': True\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "\n",
        "print_opts(args)\n",
        "G_XtoY, G_YtoX, D_X, D_Y = train(args)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                             image_size: 32                                     \n",
            "                             g_conv_dim: 32                                     \n",
            "                             d_conv_dim: 32                                     \n",
            "                            train_iters: 10000                                  \n",
            "                                      X: Apple                                  \n",
            "                                      Y: Windows                                \n",
            "                           lambda_cycle: 0.015                                  \n",
            "                                     lr: 0.0003                                 \n",
            "                                  beta1: 0.3                                    \n",
            "                                  beta2: 0.999                                  \n",
            "                             batch_size: 32                                     \n",
            "                         checkpoint_dir: checkpoints_cyclegan                   \n",
            "                             sample_dir: samples_cyclegan                       \n",
            "                               log_step: 500                                    \n",
            "                           sample_every: 500                                    \n",
            "                       checkpoint_every: 1000                                   \n",
            "                                    gpu: 1                                      \n",
            "================================================================================\n",
            "                 G_XtoY                \n",
            "---------------------------------------\n",
            "CycleGenerator(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (resnet_block): ResnetBlock(\n",
            "    (conv_layer): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (upconv1): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (1): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (upconv2): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (1): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "  )\n",
            ")\n",
            "---------------------------------------\n",
            "                 G_YtoX                \n",
            "---------------------------------------\n",
            "CycleGenerator(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (resnet_block): ResnetBlock(\n",
            "    (conv_layer): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (upconv1): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (1): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (upconv2): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (1): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "  )\n",
            ")\n",
            "---------------------------------------\n",
            "                  D_X                  \n",
            "---------------------------------------\n",
            "DCDiscriminator(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "  )\n",
            ")\n",
            "---------------------------------------\n",
            "                  D_Y                  \n",
            "---------------------------------------\n",
            "DCDiscriminator(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "  )\n",
            ")\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration [  500/10000] | d_real_loss: 0.2864 | d_Y_loss: 0.0536 | d_X_loss: 0.2288 | d_fake_loss: 0.2825 | g_loss: 1.6685\n",
            "Saved samples_cyclegan/sample-000500-X-Y.png\n",
            "Saved samples_cyclegan/sample-000500-Y-X.png\n",
            "Iteration [ 1000/10000] | d_real_loss: 0.0659 | d_Y_loss: 0.0269 | d_X_loss: 0.0553 | d_fake_loss: 0.0822 | g_loss: 0.8758\n",
            "Saved samples_cyclegan/sample-001000-X-Y.png\n",
            "Saved samples_cyclegan/sample-001000-Y-X.png\n",
            "Iteration [ 1500/10000] | d_real_loss: 0.2617 | d_Y_loss: 0.0687 | d_X_loss: 0.0582 | d_fake_loss: 0.1269 | g_loss: 1.7610\n",
            "Saved samples_cyclegan/sample-001500-X-Y.png\n",
            "Saved samples_cyclegan/sample-001500-Y-X.png\n",
            "Iteration [ 2000/10000] | d_real_loss: 0.0296 | d_Y_loss: 0.1034 | d_X_loss: 0.0194 | d_fake_loss: 0.1228 | g_loss: 1.6316\n",
            "Saved samples_cyclegan/sample-002000-X-Y.png\n",
            "Saved samples_cyclegan/sample-002000-Y-X.png\n",
            "Iteration [ 2500/10000] | d_real_loss: 0.1200 | d_Y_loss: 0.0592 | d_X_loss: 0.0980 | d_fake_loss: 0.1572 | g_loss: 0.6668\n",
            "Saved samples_cyclegan/sample-002500-X-Y.png\n",
            "Saved samples_cyclegan/sample-002500-Y-X.png\n",
            "Iteration [ 3000/10000] | d_real_loss: 0.0552 | d_Y_loss: 0.0310 | d_X_loss: 0.0282 | d_fake_loss: 0.0592 | g_loss: 0.8175\n",
            "Saved samples_cyclegan/sample-003000-X-Y.png\n",
            "Saved samples_cyclegan/sample-003000-Y-X.png\n",
            "Iteration [ 3500/10000] | d_real_loss: 0.1641 | d_Y_loss: 0.2234 | d_X_loss: 0.0198 | d_fake_loss: 0.2432 | g_loss: 3.0350\n",
            "Saved samples_cyclegan/sample-003500-X-Y.png\n",
            "Saved samples_cyclegan/sample-003500-Y-X.png\n",
            "Iteration [ 4000/10000] | d_real_loss: 0.1251 | d_Y_loss: 0.0276 | d_X_loss: 0.0479 | d_fake_loss: 0.0755 | g_loss: 1.3062\n",
            "Saved samples_cyclegan/sample-004000-X-Y.png\n",
            "Saved samples_cyclegan/sample-004000-Y-X.png\n",
            "Iteration [ 4500/10000] | d_real_loss: 0.0334 | d_Y_loss: 0.0384 | d_X_loss: 0.0214 | d_fake_loss: 0.0598 | g_loss: 1.4274\n",
            "Saved samples_cyclegan/sample-004500-X-Y.png\n",
            "Saved samples_cyclegan/sample-004500-Y-X.png\n",
            "Iteration [ 5000/10000] | d_real_loss: 0.0959 | d_Y_loss: 0.0220 | d_X_loss: 0.0741 | d_fake_loss: 0.0961 | g_loss: 1.1124\n",
            "Saved samples_cyclegan/sample-005000-X-Y.png\n",
            "Saved samples_cyclegan/sample-005000-Y-X.png\n",
            "Iteration [ 5500/10000] | d_real_loss: 0.0725 | d_Y_loss: 0.0174 | d_X_loss: 0.0716 | d_fake_loss: 0.0890 | g_loss: 0.8604\n",
            "Saved samples_cyclegan/sample-005500-X-Y.png\n",
            "Saved samples_cyclegan/sample-005500-Y-X.png\n",
            "Iteration [ 6000/10000] | d_real_loss: 0.0322 | d_Y_loss: 0.0175 | d_X_loss: 0.0118 | d_fake_loss: 0.0293 | g_loss: 0.9879\n",
            "Saved samples_cyclegan/sample-006000-X-Y.png\n",
            "Saved samples_cyclegan/sample-006000-Y-X.png\n",
            "Iteration [ 6500/10000] | d_real_loss: 0.0334 | d_Y_loss: 0.0336 | d_X_loss: 0.0204 | d_fake_loss: 0.0540 | g_loss: 0.7023\n",
            "Saved samples_cyclegan/sample-006500-X-Y.png\n",
            "Saved samples_cyclegan/sample-006500-Y-X.png\n",
            "Iteration [ 7000/10000] | d_real_loss: 0.0586 | d_Y_loss: 0.0117 | d_X_loss: 0.0197 | d_fake_loss: 0.0314 | g_loss: 1.1526\n",
            "Saved samples_cyclegan/sample-007000-X-Y.png\n",
            "Saved samples_cyclegan/sample-007000-Y-X.png\n",
            "Iteration [ 7500/10000] | d_real_loss: 0.0624 | d_Y_loss: 0.0396 | d_X_loss: 0.0266 | d_fake_loss: 0.0662 | g_loss: 0.7083\n",
            "Saved samples_cyclegan/sample-007500-X-Y.png\n",
            "Saved samples_cyclegan/sample-007500-Y-X.png\n",
            "Iteration [ 8000/10000] | d_real_loss: 0.0318 | d_Y_loss: 0.0086 | d_X_loss: 0.0236 | d_fake_loss: 0.0322 | g_loss: 1.0490\n",
            "Saved samples_cyclegan/sample-008000-X-Y.png\n",
            "Saved samples_cyclegan/sample-008000-Y-X.png\n",
            "Iteration [ 8500/10000] | d_real_loss: 0.0576 | d_Y_loss: 0.0096 | d_X_loss: 0.0192 | d_fake_loss: 0.0288 | g_loss: 0.9792\n",
            "Saved samples_cyclegan/sample-008500-X-Y.png\n",
            "Saved samples_cyclegan/sample-008500-Y-X.png\n",
            "Iteration [ 9000/10000] | d_real_loss: 0.1280 | d_Y_loss: 0.0100 | d_X_loss: 0.0342 | d_fake_loss: 0.0442 | g_loss: 0.9352\n",
            "Saved samples_cyclegan/sample-009000-X-Y.png\n",
            "Saved samples_cyclegan/sample-009000-Y-X.png\n",
            "Iteration [ 9500/10000] | d_real_loss: 0.0259 | d_Y_loss: 0.0227 | d_X_loss: 0.0612 | d_fake_loss: 0.0839 | g_loss: 0.7753\n",
            "Saved samples_cyclegan/sample-009500-X-Y.png\n",
            "Saved samples_cyclegan/sample-009500-Y-X.png\n",
            "Iteration [10000/10000] | d_real_loss: 0.1145 | d_Y_loss: 0.0190 | d_X_loss: 0.0563 | d_fake_loss: 0.0753 | g_loss: 1.1414\n",
            "Saved samples_cyclegan/sample-010000-X-Y.png\n",
            "Saved samples_cyclegan/sample-010000-Y-X.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPCiaX2IOnu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}